{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR_clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install -U -q PyDrive --quiet \n",
        "!apt install openjdk-8-jdk-headless &> /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb_gqi6dQgkM",
        "outputId": "f5b569ca-0796-4a11-cb69-ebcac722b1bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 58.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuJTnOxTQqPA",
        "outputId": "1c4afb30-7947-49de-b3f1-77eed5f9524a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O27OLoKLQDqi"
      },
      "outputs": [],
      "source": [
        "cleaned_data = sc.read \\\n",
        "  .option('header', 'True')\\\n",
        "  .option('inferSchema', 'True')\\\n",
        "  .option('sep', ',')\\\n",
        "  .csv('/content/drive/MyDrive/cleaned_data_without_duplicate.csv')\n",
        "  # .csv('/content/drive/MyDrive/cleaned_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-MsTFvwh1td",
        "outputId": "b9e21cb3-04f7-4775-f1e6-f198ab3dcf88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7630"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# balancing dataset"
      ],
      "metadata": {
        "id": "kwXqC3-Ghfm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, array, lit\n",
        "\n",
        "major_df = cleaned_data.filter(col(\"label\") == 'No')\n",
        "minor_df = cleaned_data.filter(col(\"label\") == 'Yes')\n",
        "ratio = int(major_df.count()/minor_df.count())\n",
        "print(\"ratio: {}\".format(ratio))\n",
        "a = range(ratio)\n",
        "\n",
        "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
        "\n",
        "combined_df = major_df.unionAll(oversampled_df)\n",
        "\n",
        "cleaned_data = combined_df"
      ],
      "metadata": {
        "id": "M_T4fFcPAend",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5b548e-8300-46dd-b121-e85b687cf4e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ratio: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = cleaned_data.drop('gender')\n",
        "cleaned_data = cleaned_data.drop('_c0')\n",
        "cleaned_data=cleaned_data.where((col('SeniorCitizen') != 14.0) | (col('SeniorCitizen') != 17.0) )\n",
        "cleaned_data=cleaned_data.where(cleaned_data['tenure']> 0)"
      ],
      "metadata": {
        "id": "x7rUCypgAfwi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXRYZaZKilQ0",
        "outputId": "bcd145c1-37ef-4647-ad90-cf0d1987e5bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9562"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
        "                                OneHotEncoder,StringIndexer, StandardScaler)\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "stages = []\n",
        "\n",
        "numeric_features = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "# numeric_features = ['SeniorCitizen', 'tenure', 'TotalCharges']\n",
        "categorical_columns =  [item[0] for item in cleaned_data.dtypes if item[1].startswith('string')]\n",
        "categorical_columns.remove('Label')\n",
        "\n",
        "for feature in categorical_columns:\n",
        "  stringIndexer = StringIndexer(inputCol=feature, outputCol=feature + 'Index')\n",
        "  encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],outputCols=[feature + 'Vec'])\n",
        "  stages+=[stringIndexer, encoder]\n",
        "\n",
        "labelIndexer = StringIndexer(inputCol='Label', outputCol='label')\n",
        "stages += [labelIndexer]\n",
        "input_features = [c + 'Vec' for c in categorical_columns]+ numeric_features\n",
        "assembler = VectorAssembler(inputCols= input_features, outputCol='features')\n",
        "stages+=[assembler]\n",
        "scaler = StandardScaler(inputCol='features', outputCol='standard_features')\n",
        "stages+=[scaler]\n",
        "\n",
        "pipeline = Pipeline(stages=stages)\n",
        "pipelineModel = pipeline.fit(cleaned_data)\n",
        "assembler_df = pipelineModel.transform(cleaned_data)"
      ],
      "metadata": {
        "id": "wDxexNC_QI97"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training model"
      ],
      "metadata": {
        "id": "zdkOjefLo8XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "log_reg= LogisticRegression(featuresCol='features',labelCol='label')\n",
        "\n",
        "train_data, test_data = assembler_df.randomSplit([0.8, 0.2])\n",
        "fit_model = log_reg.fit(train_data)\n",
        "results = fit_model.transform(test_data)\n",
        "\n",
        "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
        "                                       labelCol='label')\n",
        "results.select('label','prediction')\n",
        "AUC = my_eval.evaluate(results)\n",
        "print(\"AUC score is : \",AUC)\n",
        "\n",
        "accuracy = results.filter(results.label == results.prediction).count() / float(results.count())\n",
        "print(\"Accuracy : \",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG9wP1HFQKlA",
        "outputId": "3b2ee172-48cd-46a5-e58f-52191ee24fd5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC score is :  0.8489109148695582\n",
            "Accuracy :  0.8340471092077087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TN = results.filter('prediction = 0 AND Label = prediction').count()\n",
        "TP = results.filter('prediction = 1 AND Label = prediction').count()\n",
        "FN = results.filter('prediction = 0 AND Label = 1').count()\n",
        "FP = results.filter('prediction = 1 AND Label = 0').count()\n",
        "Precision = TP/(TP+FP)\n",
        "Recall = TP/(TP+FN)\n",
        "F1=(2*Precision*Recall)/(Precision+Recall)\n",
        "print(\"Precision : \"+str(Precision))\n",
        "print(\"Recall: : \"+str(Recall))\n",
        "print(\"F1: \"+str(F1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "910rC6O_ivLk",
        "outputId": "c5e27c2f-0cf1-45a2-91ab-6f11cefab9bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision : 0.7335423197492164\n",
            "Recall: : 0.9273447820343461\n",
            "F1: 0.8191365227537923\n"
          ]
        }
      ]
    }
  ]
}